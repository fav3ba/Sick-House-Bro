{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sick House Bro\n",
    "## Authors: Frank Vasquez (fav3ba), June Suh (kqj8be), Kevin Lin (pex7ps)\n",
    "### Goal: Determine if the pandemic had a direct impact on the housing market, and see if trends are discoverable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "First covid cases in US were confirmed in January 2020\n",
    "\n",
    "For weekly assignment:\n",
    "At a minimum, the file should include a summary containing:\n",
    "\n",
    "Number of records: 38,475 \\\n",
    "Number of columns: 2 predictors, 1 response, 1 col to group by county, 1 col to group by date\\\n",
    "Statistical summary of response variable (at the bottom) \\\n",
    "Statistical summary of potential predictor variables (at the bottom)\\\n",
    "Include up to five helpful graphs (at the borttom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data_dir = '../data/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FIPS to county crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fips to county read in and add columns to be compatible with other data\n",
    "\n",
    "schema = StructType([StructField(\"state\", StringType(), False),\n",
    "                     StructField(\"state_fips\", StringType(), False),\n",
    "                     StructField(\"county_fips\", StringType(), False),\n",
    "                     StructField(\"county_name\", StringType(), False),\n",
    "                     StructField(\"H\", StringType(), False)])\n",
    "\n",
    "fips2county = spark.read.csv(f\"{data_dir}/fips2county.txt\",  schema)\n",
    "fips2county = fips2county.withColumn('full_fips', concat(fips2county.state_fips,fips2county.county_fips)) \\\n",
    "                .withColumn('full_county', concat(fips2county.county_name, lit(', '), fips2county.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+--------------+---+---------+------------------+\n",
      "|state|state_fips|county_fips|   county_name|  H|full_fips|       full_county|\n",
      "+-----+----------+-----------+--------------+---+---------+------------------+\n",
      "|   AL|        01|        001|Autauga County| H1|    01001|Autauga County, AL|\n",
      "|   AL|        01|        003|Baldwin County| H1|    01003|Baldwin County, AL|\n",
      "|   AL|        01|        005|Barbour County| H1|    01005|Barbour County, AL|\n",
      "|   AL|        01|        007|   Bibb County| H1|    01007|   Bibb County, AL|\n",
      "|   AL|        01|        009| Blount County| H1|    01009| Blount County, AL|\n",
      "+-----+----------+-----------+--------------+---+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fips2county.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Median house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer schema does not work here, too many columns to manually type\n",
    "# using pandas to cut down dataframe to useful part, manual schema to convert to spark data frame\n",
    "\n",
    "house = pd.read_csv(f\"{data_dir}/med_sale_price_counties.csv\", header=1) # import with pandas to skip first row\n",
    "house.shape # 1860 counties, 120 months, only want last 24 (2020-2021)\n",
    "house_county = pd.DataFrame(house.iloc[:,0]) # grab first column\n",
    "house_pand = house.iloc[:, -24:] # grab 2020 and 2021\n",
    "new_house = house_county.join(house_pand) # append first column to subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create schema from list of columns to get around improper schema inference\n",
    "schema_list = []\n",
    "for i in new_house.columns:\n",
    "    schema_list.append(StructField(i, StringType(), False))\n",
    "schema = StructType(schema_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark data frame using schema built from column names\n",
    "house = spark.createDataFrame(new_house, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. COVID case and death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in covid positive and death data\n",
    "\n",
    "case_data =  spark.read.csv(f\"{data_dir}/time_series_covid19_confirmed_US.csv\", inferSchema=True, header=True)\n",
    "death_data = spark.read.csv(f\"{data_dir}/time_series_covid19_deaths_US.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fips codes to strings\n",
    "case_data = case_data.withColumn(\"UID\",case_data[\"UID\"].cast('string'))\n",
    "death_data = death_data.withColumn(\"UID\",death_data[\"UID\"].cast('string'))\n",
    "\n",
    "# create 5 digit fips columns\n",
    "case_data = case_data.withColumn('FIPS', case_data['UID'][4:8])\n",
    "death_data = death_data.withColumn('FIPS', death_data['UID'][4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month labels for aggregation of cases and deaths\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\",\"Nov\",\"Dec\"]\n",
    "years = ['20','21'] #,'2022'] add back in if wanting 2022 data\n",
    "combined = []\n",
    "for x in months:\n",
    "    for y in years:\n",
    "        combined.append(x+\"-\"+y) # using same naming scheme as housing data\n",
    "\n",
    "# add columns for aggregation\n",
    "# there's probably a way to do this all at once instead of as a loop\n",
    "for date in combined:\n",
    "    case_data = case_data.withColumn(date, lit(0))\n",
    "    death_data = death_data.withColumn(date, lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to month columns\n",
    "# there's gotta be a better way to do this but I can't think of it\n",
    "# convert to pandas dataframe for aggregation step, faster due to communication time\n",
    "\n",
    "death_schema = death_data.schema\n",
    "case_schema = case_data.schema\n",
    "\n",
    "death_data = death_data.toPandas()\n",
    "case_data = case_data.toPandas()\n",
    "\n",
    "# death data\n",
    "for i in range(0, len(death_data.columns)):\n",
    "    col_name = death_data.columns[i]\n",
    "    temp = col_name.split('/')\n",
    "    if(len(temp) == 3): # makes sure to only work on date columns\n",
    "        temp = [int(i) for i in temp] \n",
    "        if(temp[2] == 20):\n",
    "            if(temp[0] == 1): death_data['Jan-20'] = death_data['Jan-20']+death_data[col_name]\n",
    "            if(temp[0] == 2): death_data['Feb-20'] = death_data['Feb-20']+death_data[col_name]\n",
    "            if(temp[0] == 3): death_data['Mar-20'] = death_data['Mar-20']+death_data[col_name]\n",
    "            if(temp[0] == 4): death_data['Apr-20'] = death_data['Apr-20']+death_data[col_name]\n",
    "            if(temp[0] == 5): death_data['May-20'] = death_data['May-20']+death_data[col_name]\n",
    "            if(temp[0] == 6): death_data['Jun-20'] = death_data['Jun-20']+death_data[col_name]\n",
    "            if(temp[0] == 7): death_data['Jul-20'] = death_data['Jul-20']+death_data[col_name]\n",
    "            if(temp[0] == 8): death_data['Aug-20'] = death_data['Aug-20']+death_data[col_name]\n",
    "            if(temp[0] == 9): death_data['Sep-20'] = death_data['Sep-20']+death_data[col_name]\n",
    "            if(temp[0] == 10): death_data['Oct-20'] = death_data['Oct-20']+death_data[col_name]\n",
    "            if(temp[0] == 11): death_data['Nov-20'] = death_data['Nov-20']+death_data[col_name]\n",
    "            if(temp[0] == 12): death_data['Dec-20'] = death_data['Dec-20']+death_data[col_name]\n",
    "        if(temp[2] == 21):\n",
    "            if(temp[0] == 1): death_data['Jan-21'] = death_data['Jan-21']+death_data[col_name]\n",
    "            if(temp[0] == 2): death_data['Feb-21'] = death_data['Feb-21']+death_data[col_name]\n",
    "            if(temp[0] == 3): death_data['Mar-21'] = death_data['Mar-21']+death_data[col_name]\n",
    "            if(temp[0] == 4): death_data['Apr-21'] = death_data['Apr-21']+death_data[col_name]\n",
    "            if(temp[0] == 5): death_data['May-21'] = death_data['May-21']+death_data[col_name]\n",
    "            if(temp[0] == 6): death_data['Jun-21'] = death_data['Jun-21']+death_data[col_name]\n",
    "            if(temp[0] == 7): death_data['Jul-21'] = death_data['Jul-21']+death_data[col_name]\n",
    "            if(temp[0] == 8): death_data['Aug-21'] = death_data['Aug-21']+death_data[col_name]\n",
    "            if(temp[0] == 9): death_data['Sep-21'] = death_data['Sep-21']+death_data[col_name]\n",
    "            if(temp[0] == 10): death_data['Oct-21'] = death_data['Oct-21']+death_data[col_name]\n",
    "            if(temp[0] == 11): death_data['Nov-21'] = death_data['Nov-21']+death_data[col_name]\n",
    "            if(temp[0] == 12): death_data['Dec-21'] = death_data['Dec-21']+death_data[col_name] # don't have data after this for houses\n",
    "        #if(temp[2] == 22):\n",
    "         #   if(temp[0] == 1): death_data['Jan-2022'] = death_data['Jan-2022']+death_data[col_name]\n",
    "          #  if(temp[0] == 2): death_data['Feb-2022'] = death_data['Feb-2022']+death_data[col_name] # data after this won't exist\n",
    "                \n",
    "# case data\n",
    "for i in range(0, len(case_data.columns)):\n",
    "    col_name = case_data.columns[i]\n",
    "    temp = col_name.split('/')\n",
    "    if(len(temp) == 3): # makes sure to only work on date columns\n",
    "        temp = [int(i) for i in temp] \n",
    "        if(temp[2] == 20):\n",
    "            if(temp[0] == 1): case_data['Jan-20'] = case_data['Jan-20']+case_data[col_name]\n",
    "            if(temp[0] == 2): case_data['Feb-20'] = case_data['Feb-20']+case_data[col_name]\n",
    "            if(temp[0] == 3): case_data['Mar-20'] = case_data['Mar-20']+case_data[col_name]\n",
    "            if(temp[0] == 4): case_data['Apr-20'] = case_data['Apr-20']+case_data[col_name]\n",
    "            if(temp[0] == 5): case_data['May-20'] = case_data['May-20']+case_data[col_name]\n",
    "            if(temp[0] == 6): case_data['Jun-20'] = case_data['Jun-20']+case_data[col_name]\n",
    "            if(temp[0] == 7): case_data['Jul-20'] = case_data['Jul-20']+case_data[col_name]\n",
    "            if(temp[0] == 8): case_data['Aug-20'] = case_data['Aug-20']+case_data[col_name]\n",
    "            if(temp[0] == 9): case_data['Sep-20'] = case_data['Sep-20']+case_data[col_name]\n",
    "            if(temp[0] == 10): case_data['Oct-20'] = case_data['Oct-20']+case_data[col_name]\n",
    "            if(temp[0] == 11): case_data['Nov-20'] = case_data['Nov-20']+case_data[col_name]\n",
    "            if(temp[0] == 12): case_data['Dec-20'] = case_data['Dec-20']+case_data[col_name]\n",
    "        if(temp[2] == 21):\n",
    "            if(temp[0] == 1): case_data['Jan-21'] = case_data['Jan-21']+case_data[col_name]\n",
    "            if(temp[0] == 2): case_data['Feb-21'] = case_data['Feb-21']+case_data[col_name]\n",
    "            if(temp[0] == 3): case_data['Mar-21'] = case_data['Mar-21']+case_data[col_name]\n",
    "            if(temp[0] == 4): case_data['Apr-21'] = case_data['Apr-21']+case_data[col_name]\n",
    "            if(temp[0] == 5): case_data['May-21'] = case_data['May-21']+case_data[col_name]\n",
    "            if(temp[0] == 6): case_data['Jun-21'] = case_data['Jun-21']+case_data[col_name]\n",
    "            if(temp[0] == 7): case_data['Jul-21'] = case_data['Jul-21']+case_data[col_name]\n",
    "            if(temp[0] == 8): case_data['Aug-21'] = case_data['Aug-21']+case_data[col_name]\n",
    "            if(temp[0] == 9): case_data['Sep-21'] = case_data['Sep-21']+case_data[col_name]\n",
    "            if(temp[0] == 10): case_data['Oct-21'] = case_data['Oct-21']+case_data[col_name]\n",
    "            if(temp[0] == 11): case_data['Nov-21'] = case_data['Nov-21']+case_data[col_name]\n",
    "            if(temp[0] == 12): case_data['Dec-21'] = case_data['Dec-21']+case_data[col_name] # don't have data after this for houses\n",
    "     #   if(temp[2] == 22):\n",
    "      #      if(temp[0] == 1): case_data['Jan-2022'] = case_data['Jan-2022']+case_data[col_name]\n",
    "       #     if(temp[0] == 2): case_data['Feb-2022'] = case_data['Feb-2022']+case_data[col_name] # data after this won't exist\n",
    "    \n",
    "    \n",
    "# return to spark dataframes\n",
    "death_data = spark.createDataFrame(death_data, schema=death_schema)\n",
    "case_data = spark.createDataFrame(case_data, schema=case_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only relevant columns\n",
    "combined.append('FIPS') # for making new DFs with only relevant columns\n",
    "death_data_clean = death_data.select(*combined)\n",
    "case_data_clean = case_data.select(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to do some creative pandas manipulation to create features from data\n",
    "Unstack is a pandas specific operation, and Spark DFs do not use indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fips to house data frame\n",
    "fips2county = fips2county.withColumnRenamed(\"full_county\",\"Region\")\n",
    "house = house.join(fips2county,['Region'],how='outer')\n",
    "house = house.withColumnRenamed('full_fips','FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas\n",
    "house = house.toPandas()\n",
    "\n",
    "# drop unnecessary columns and index by FIPS and date\n",
    "house.index = house.FIPS\n",
    "house = house.drop(['Region','state','state_fips','county_fips','county_name','H','FIPS'], axis=1) # drops extra columns\n",
    "house_date_fips = pd.DataFrame(house.unstack(), columns=['median_home_value']) # indexes by date and FIPS\n",
    "house_date_fips = house_date_fips.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "case_data_clean = case_data_clean.toPandas()\n",
    "death_data_clean = death_data_clean.toPandas()\n",
    "\n",
    "# change index to FIPS and drop FIPS column\n",
    "case_data_clean.index = case_data_clean.FIPS\n",
    "case_data_clean = case_data_clean.drop('FIPS', axis=1)\n",
    "death_data_clean.index = death_data_clean.FIPS\n",
    "death_data_clean = death_data_clean.drop('FIPS', axis=1)\n",
    "\n",
    "# index death and case data by date\n",
    "case_date_fips = pd.DataFrame(case_data_clean.unstack(), columns=['confirmed_cases'])\n",
    "death_date_fips = pd.DataFrame(death_data_clean.unstack(), columns=['confirmed_deaths'])\n",
    "case_date_fips = case_date_fips.dropna()\n",
    "death_date_fips = death_date_fips.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>confirmed_cases</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Jan-20</th>\n",
       "      <th>01001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01005</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01007</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01009</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Dec-21</th>\n",
       "      <th>56039</th>\n",
       "      <td>171435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56041</th>\n",
       "      <td>127294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90056</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56043</th>\n",
       "      <td>57729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>38058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              confirmed_cases\n",
       "       FIPS                  \n",
       "Jan-20 01001                0\n",
       "       01003                0\n",
       "       01005                0\n",
       "       01007                0\n",
       "       01009                0\n",
       "...                       ...\n",
       "Dec-21 56039           171435\n",
       "       56041           127294\n",
       "       90056                0\n",
       "       56043            57729\n",
       "       56045            38058\n",
       "\n",
       "[80208 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_date_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              confirmed_deaths     date_fips\n",
      "       FIPS                                 \n",
      "Jan-20 01001                 0  Jan-20-01001\n",
      "       01003                 0  Jan-20-01003\n",
      "       01005                 0  Jan-20-01005\n",
      "       01007                 0  Jan-20-01007\n",
      "       01009                 0  Jan-20-01009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              confirmed_cases     date_fips\n",
      "       FIPS                                \n",
      "Jan-20 01001                0  Jan-20-01001\n",
      "       01003                0  Jan-20-01003\n",
      "       01005                0  Jan-20-01005\n",
      "       01007                0  Jan-20-01007\n",
      "       01009                0  Jan-20-01009\n",
      "             median_home_value     date_fips\n",
      "       FIPS                                 \n",
      "Jan-20 47039               NaN  Jan-20-47039\n",
      "       18077             $125K  Jan-20-18077\n",
      "       42069             $112K  Jan-20-42069\n",
      "       32015             $152K  Jan-20-32015\n",
      "       18109             $170K  Jan-20-18109\n"
     ]
    }
   ],
   "source": [
    "# create column to join on for all dfs from indices\n",
    "\n",
    "death_date_fips['date_fips'] = \"\"\n",
    "for i in range(0,len(death_date_fips)):\n",
    "    death_date_fips['date_fips'][i] = death_date_fips.index[i][0]+'-'+death_date_fips.index[i][1]\n",
    "\n",
    "print(death_date_fips.head())\n",
    "    \n",
    "case_date_fips['date_fips'] = \"\"\n",
    "for i in range(0,len(case_date_fips)):\n",
    "    case_date_fips['date_fips'][i] = case_date_fips.index[i][0]+'-'+case_date_fips.index[i][1]\n",
    "    \n",
    "print(case_date_fips.head())\n",
    "    \n",
    "house_date_fips['date_fips'] = \"\"\n",
    "for i in range(0,len(house_date_fips)):\n",
    "    house_date_fips['date_fips'][i] = str(house_date_fips.index[i][0])+'-'+str(house_date_fips.index[i][1])\n",
    "                                          \n",
    "print(house_date_fips.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to Spark\n",
    "house_schema = StructType([StructField(\"median_home_value\", StringType(), False),\n",
    "                           StructField(\"date_fips\", StringType(), False)])\n",
    "death_schema = StructType([StructField(\"confirmed_deaths\", IntegerType(), False),\n",
    "                           StructField(\"date_fips\", StringType(), False)])\n",
    "case_schema = StructType([StructField(\"confirmed_cases\", IntegerType(), False),\n",
    "                          StructField(\"date_fips\", StringType(), False)])\n",
    "\n",
    "house = spark.createDataFrame(house_date_fips, schema=house_schema)\n",
    "death_data = spark.createDataFrame(death_date_fips, schema=death_schema)\n",
    "case_data = spark.createDataFrame(case_date_fips, schema=case_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_fips='Apr-20-17083', median_home_value='$175K', confirmed_deaths=6, confirmed_cases=214),\n",
       " Row(date_fips='Apr-20-39029', median_home_value='$85K', confirmed_deaths=452, confirmed_cases=4061),\n",
       " Row(date_fips='Apr-20-41059', median_home_value='$230K', confirmed_deaths=0, confirmed_cases=630),\n",
       " Row(date_fips='Apr-20-42063', median_home_value='$145K', confirmed_deaths=58, confirmed_cases=1185),\n",
       " Row(date_fips='Apr-20-47139', median_home_value='$260K', confirmed_deaths=0, confirmed_cases=142)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dfs and drop NA\n",
    "\n",
    "combined_df = house.join(death_data,['date_fips'],how='outer')\n",
    "combined_df = combined_df.join(case_data,['date_fips'],how='outer')\n",
    "combined_df = combined_df.dropna()\n",
    "combined_df = combined_df.filter(combined_df.median_home_value != 'NaN') # some NaN values converted to strings\n",
    "combined_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_fips='Apr-20-17083', confirmed_deaths=6, confirmed_cases=214, median_home_value=175000),\n",
       " Row(date_fips='Apr-20-39029', confirmed_deaths=452, confirmed_cases=4061, median_home_value=85000),\n",
       " Row(date_fips='Apr-20-41059', confirmed_deaths=0, confirmed_cases=630, median_home_value=230000),\n",
       " Row(date_fips='Apr-20-42063', confirmed_deaths=58, confirmed_cases=1185, median_home_value=145000),\n",
       " Row(date_fips='Apr-20-47139', confirmed_deaths=0, confirmed_cases=142, median_home_value=260000)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# convert home value to dollars\n",
    "\n",
    "med_val = combined_df.select('median_home_value').rdd.flatMap(lambda x: x).collect() # convert to list\n",
    "med_val = [x[1:-1] for x in med_val]\n",
    "med_val = [int(x.replace(',', '')) for x in med_val] # convert to int and handle commas\n",
    "med_val = [x*1000 for x in med_val]\n",
    "med_val_df = spark.createDataFrame([(l,) for l in med_val], ['median_home_value'])\n",
    "\n",
    "# make new median_home_value columns (might be a shorter way to do this)\n",
    "combined_df = combined_df.drop('median_home_value')\n",
    "combined_df = combined_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "med_val_df = med_val_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "combined_df = combined_df.join(med_val_df, combined_df.row_idx == med_val_df.row_idx).drop(\"row_idx\")\n",
    "\n",
    "combined_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_fips='Apr-20-17083', confirmed_deaths=6, confirmed_cases=214, median_home_value=175000, fips='17083', date='Apr-20'),\n",
       " Row(date_fips='Apr-20-39029', confirmed_deaths=452, confirmed_cases=4061, median_home_value=85000, fips='39029', date='Apr-20'),\n",
       " Row(date_fips='Apr-20-41059', confirmed_deaths=0, confirmed_cases=630, median_home_value=230000, fips='41059', date='Apr-20'),\n",
       " Row(date_fips='Apr-20-42063', confirmed_deaths=58, confirmed_cases=1185, median_home_value=145000, fips='42063', date='Apr-20'),\n",
       " Row(date_fips='Apr-20-47139', confirmed_deaths=0, confirmed_cases=142, median_home_value=260000, fips='47139', date='Apr-20')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull out fips code and date separately\n",
    "dateFips = combined_df.select('date_fips').rdd.flatMap(lambda x: x).collect()\n",
    "fips = [x[-5:] for x in dateFips]\n",
    "date = [x[:6] for x in dateFips]\n",
    "\n",
    "fips_df = spark.createDataFrame([(l,) for l in fips], ['fips'])\n",
    "date_df = spark.createDataFrame([(l,) for l in date], ['date'])\n",
    "\n",
    "combined_df = combined_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "fips_df = fips_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "combined_df = combined_df.join(fips_df, combined_df.row_idx == fips_df.row_idx).drop('row_idx')\n",
    "combined_df = combined_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "date_df = date_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "combined_df = combined_df.join(date_df, combined_df.row_idx == date_df.row_idx).drop('row_idx')\n",
    "\n",
    "combined_df.take(5)\n",
    "# this is where I stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   confirmed_cases|\n",
      "+-------+------------------+\n",
      "|  count|             38475|\n",
      "|   mean| 319111.4653411306|\n",
      "| stddev|1236091.6039068876|\n",
      "|    min|                 0|\n",
      "|    max|          48717613|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictor 1\n",
    "combined_df.describe(['confirmed_cases']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary| confirmed_deaths|\n",
      "+-------+-----------------+\n",
      "|  count|            38475|\n",
      "|   mean| 5733.53380116959|\n",
      "| stddev|22782.59565112056|\n",
      "|    min|                0|\n",
      "|    max|           849402|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictor 2\n",
    "combined_df.describe(['confirmed_deaths']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary| median_home_value|\n",
      "+-------+------------------+\n",
      "|  count|             38475|\n",
      "|   mean| 251515.1397011046|\n",
      "| stddev|186257.69664910855|\n",
      "|    min|                 0|\n",
      "|    max|          12250000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# response\n",
    "combined_df.describe(['median_home_value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['confirmed_cases','confirmed_deaths']\n",
    "combined_df = combined_df.withColumn('label', combined_df.median_home_value) # for tree classifiers\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feats,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+---------------+-----------------+-----+------+------+--------------+\n",
      "|   date_fips|confirmed_deaths|confirmed_cases|median_home_value| fips|  date| label|      features|\n",
      "+------------+----------------+---------------+-----------------+-----+------+------+--------------+\n",
      "|Apr-20-17083|               6|            214|           175000|17083|Apr-20|175000|   [214.0,6.0]|\n",
      "|Apr-20-39029|             452|           4061|            85000|39029|Apr-20| 85000|[4061.0,452.0]|\n",
      "|Apr-20-41059|               0|            630|           230000|41059|Apr-20|230000|   [630.0,0.0]|\n",
      "+------------+----------------+---------------+-----------------+-----+------+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 69\n",
    "train_test = [0.8, 0.2]\n",
    "train_data, test_data = tr.randomSplit(train_test, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have a reason for choosing these parameters other than it's the default from homeworks and lecture. Open to change.\n",
    "\n",
    "lr = LinearRegression(featuresCol='features',         \n",
    "                      labelCol='median_home_value',  \n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol='features')\n",
    "\n",
    "dtModel = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features')\n",
    "\n",
    "rfModel = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lrModel.transform(test_data)\n",
    "\n",
    "MSE = []\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_home_value\")\n",
    "lr_mse = ev.evaluate(lr_pred, {ev.metricName: \"mse\"})\n",
    "MSE.append(lr_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dtModel.transform(test_data)\n",
    "dt_mse = ev.evaluate(dt_pred, {ev.metricName: \"mse\"})\n",
    "MSE.append(dt_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rfModel.transform(test_data)\n",
    "rf_mse = ev.evaluate(rf_pred, {ev.metricName: \"mse\"})\n",
    "MSE.append(rf_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|   regression|                 MSE|\n",
      "+-------------+--------------------+\n",
      "|       Linear|3.369250257372688E10|\n",
      "|Decision Tree| 3.18640313602778E10|\n",
      "|Random Forest|3.202095356309985E10|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression = ['Linear','Decision Tree','Random Forest']\n",
    "MSE_df = spark.createDataFrame(zip(regression,MSE), ['regression','MSE'])\n",
    "MSE_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
